% !TEX root = Econometrics.tex

\setcounter{chapter}{4}
\chapter{Normal Regression}

% \section{Introduction}

\setcounter{section}{1}
\section{The Normal Distribution}

We say that a random variable $Z$ has the \highlightB{standard normal distribution}, or \highlightB{Gaussian}, written $Z \sim N\of{0,1}$, if it has the density 
$$\phi(x)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{x^2}{2}\right), \quad -\infty < x < \infty .$$
The standard normal density is typically written with the symbol $\phi\of{x}$ and the corresponding distribution by $\Phi\of{x}$.

\begin{theorem} 
    \label{Hansen_thm_5_1}
    If $Z \sim N\of{0,1}$, then 
    \begin{enumerate}[topsep=10pt, leftmargin=20pt, itemsep=0pt, label=(\arabic*)]
        \setlength{\parskip}{10pt} 
        \item All integer moments of $Z$ are finite.
        \item All odd moments of $Z$ equal 0.
        \item For any positive integer $m$ 
        \begin{equation}
            \notag
            \E\bs{Z^{2m}} = \bp{2m-1}!! = \bp{2m-1} \times \bp{2m-3} \times \ldots \times 1.
        \end{equation}
        \item For any $r>0$,
        \begin{equation}
            \notag
            \E\bs{\abs{Z}^r} = \frac{2^{r/2}}{\sqrt{\pi}} \Gamma\of{\frac{r+1}{2}}
        \end{equation}
        where $\Gamma\of{t} = \int_0^\infty u^{t-1} e^{-u} du$ is the gamma function.
    \end{enumerate}
\end{theorem}

If $Z \sim N\of{0,1}$ and $X = \mu + \s Z$ for $\mu \in \R$ and $\s \geq 0$ then $X$ has the \highlightB{univariate normal distribution}, written $X \sim N\of{\mu, \s^2}$. $X$ has the density 
$$f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{(x-\mu)^2}{2 \sigma^2}\right), \quad-\infty<x<\infty.$$
The mean and variance of $X$ are $\mu$ and $\s^2$, respectively.

The normal distribution and its relatives (the chi-square, student $t$, $F$, non-central chi-square, and non-central $F$) are frequently used for inference to calculate critical values and $p$-values. This involves evaluating the normal cdf $\Phi\of{x}$ and its inverse. Since the cdf $\Phi\of{x}$ is not available in closed form statistical textbooks have traditionally provided tables for this purpose.


\section{Multivariate Normal Distribution}

We say that the $k$-vector $\ora{Z}$ has a \highlightB{multivariate standard normal distribution}, written $\ora{Z} \sim N\of{\ora{0}, \Ib_k}$, if it has the joint density, 

\begin{equation}
    \notag
    f\of{\ora{x}} = \frac{1}{\bp{2\pi}^{k/2}} \exp\of{-\frac{\ora{x}^\prime \ora{x}}{2}}, \quad x \in \R^k.
\end{equation}

The mean and covariance matrix of $\ora{Z}$ are $\ora{0}$ and $\Ib_k$, respectively. The multivariate joint density factors into the product of univariate normal densities, so the elements of $\ora{Z}$ are mutually independent standard normals.

If $\ora{Z} \sim N\of{\ora{0}, \Ib_k}$ and $\ora{X} = \ora{\mu} + \Bb \ora{Z}$ then the $k$-vector $\ora{X}$ has a \highlightB{multivariate normal distribution}, written $\ora{X} \sim N\of{\ora{\mu}, \bds{\S}}$, where $\bds{\S} = \Bb \Bb^\prime$. If $\bds{\S} > \bds{0}$, then by change-of-variables $\ora{X}$ has the joint density function

\begin{equation}
    \notag
    f(\ora{x})=\frac{1}{(2 \pi)^{k / 2} \operatorname{det}(\bds{\S})^{1 / 2}} \exp \left(-\frac{(\ora{x}-\ora{\mu})^{\prime} \Sigma^{-1}(\ora{x}-\ora{\mu})}{2}\right), \quad \ora{x} \in \mathbb{R}^k .
\end{equation}

An important property of normal random vectors is that affine functions are multivariate normal.

\begin{theorem}
    \label{Hansen_thm_5_2}
    If $\ora{X} \sim N\of{\ora{\mu}, \bds{\S}}$ and $Y = \ora{a} + \Bb \ora{X}$, then $\ora{Y} \sim N\of{\ora{a} + \Bb\ora{\mu}, \Bb\bds{\S}\Bb^\prime}$.
\end{theorem}

One simple implication of Theorem \ref{Hansen_thm_5_2} is that if $\ora{X}$ is multivariate normal then each component of $\ora{X}$ is univariate normal.

Another useful property of the multivariate normal distribution is that uncorrelatedness is the same as independence. \highlightR{That is, if a vector is multivariate normal, subsets of variables are independent if and only if they are uncorrelated.}

\begin{theorem}[Properties of the Multivariate Normal Distribution]
    \label{Hansen_thm_5_3}

    \begin{enumerate}[topsep=10pt, leftmargin=20pt, itemsep=0pt, label=(\arabic*)]
        \setlength{\parskip}{10pt} 
        \item The mean and covariance matrix of $\ora{X} \sim N\of{\ora{\mu}, \bds{\S}}$ are $\E\bs{\ora{X}} = \ora{\mu}$ and $\var\bs{\ora{X}} = \bds{\S}$.
        \item If $\bp{\ora{X}, \ora{Y}}$ are multivariate normal, $\ora{X}$ and $\ora{Y}$ are uncorrelated if and only if they are independent.
        \item If $\ora{X} \sim N\of{\ora{\mu}, \bds{\S}}$ and $Y = \ora{a} + \Bb \ora{X}$, then $\ora{Y} \sim N\of{\ora{a} + \Bb\ora{\mu}, \Bb\bds{\S}\Bb^\prime}$.
        \item If $\ora{X} \sim N\of{\ora{0}, \Ib_k}$ then $\ora{X}^\prime\ora{X} \sim \chi_k^2$, chi-square with $k$ degrees of freedom.
        \item If $\ora{X} \sim N\of{\ora{0}, \bds{\S}}$ with $\bds{\S} > \bds{0}$ then $\ora{X}^\prime\bds{\S}^{-1}\ora{X} \sim \chi_k^2$ where $k = \operatorname*{dim}\of{X}$.
        \item If $\ora{X} \sim N\of{\ora{\mu}, \bds{\S}}$ with $\bds{\S} > \bds{0}, r \times r$, then $\ora{X}^\prime\bds{\S}^{-1}\ora{X} \sim \chi_k^2\of{\l}$ where $\l = \ora{\mu}^\prime \bds{\S}^{-1}\ora{\mu}$.
        \item If $Z \sim N\of{0,1}$ and $Q \sim \chi_k^2$ are independent then $\frac{Z}{\sqrt{Q/k}} \sim t_k$, student $t$ with $k$ degrees of freedom.
        \item If $\bp{\ora{Y}, \ora{X}}$ are multivariate normal,
        \begin{equation}
            \notag
            \left(\begin{array}{l}
                \ora{Y} \\
                \ora{X}
                \end{array}\right) \sim \mathrm{N}\left(\left(\begin{array}{c}
                    \ora{\mu_Y} \\
                    \ora{\mu_X}
                \end{array}\right),\left(\begin{array}{cc}
                    \bds{\Sigma}_{Y Y} & \bds{\Sigma}_{Y X} \\
                    \bds{\Sigma}_{X Y} & \bds{\Sigma}_{X X}
                \end{array}\right)\right)
        \end{equation}
        where $\bds{\Sigma_{Y Y}} > \bds{0}$ and $\bds{\Sigma_{X X}} > \bds{0}$ then the conditional distributions are 
        \begin{equation}
            \notag
            \begin{aligned}
                & \ora{Y} \mid \ora{X} \sim \mathrm{N}\left(\ora{\mu_Y}+\bds{\Sigma}_{Y X} \bds{\Sigma}_{X X}^{-1}\left(\ora{X}-\ora{\mu_X}\right), \bds{\Sigma}_{Y Y}-\bds{\Sigma}_{Y X} \bds{\Sigma}_{X X}^{-1} \bds{\Sigma}_{X Y}\right) \\
                & \ora{X} \mid \ora{Y} \sim \mathrm{N}\left(\ora{\mu_X}+\bds{\Sigma}_{X Y} \bds{\Sigma}_{Y Y}^{-1}\left(\ora{Y}-\ora{\mu_Y}\right), \bds{\Sigma}_{X X}-\bds{\Sigma}_{X Y} \bds{\Sigma}_{Y Y}^{-1} \bds{\Sigma}_{Y X}\right) .
                \end{aligned}
        \end{equation}
    \end{enumerate}
\end{theorem}

\highlightPP{Multivariate normal distributions and corresponding derived distributions should be reviewed in the appendix.}

\section{Joint Normality and Linear Regression} \label{hansen_sec_5_4}

Suppose the variables $\bp{Y, \ora{X}}$ are jointly normally distributed. Consider the best linear predictor of $Y$ given $\ora{X}$ 
$$Y = \ora{X}^\prime \ora{\b} + \a + e.$$
By the properties of the best linear predictor, $\E\bs{\ora{X}e} = \ora{0}$ and $\E\bs{e} = 0$, so $\ora{X}$ and $e$ are uncorrelated. Since $\bp{e, \ora{X}}$ is an affine transformation of the normal vector $\bp{Y, \ora{X}}$ it follows that $\bp{e, \ora{X}}$ is jointly normal (Theorem \ref{Hansen_thm_5_2}). Since $\bp{e, \ora{X}}$ is jointly normal and uncorrelated they are independent (Theorem \ref{Hansen_thm_5_3}). Independence implies that 
\begin{equation}
    \notag
    \E\bs{e \mid \ora{X}} = \E\bs{e} = 0
\end{equation}
and 
\begin{equation}
    \notag
    \E\bs{e^2 \mid \ora{X}} = \E\bs{e^2} = \s^2
\end{equation}
which are properties of a homoskedastic linear CEF.

We have shown that when $\bp{Y, \ora{X}}$ are jointly normally distributed they satisfy a normal linear CEF 
$$Y = \ora{X}^\prime \ora{\b} + \a + e$$
where 
$$e \sim N\of{0, \s^2}$$
is independent of $\ora{X}$.

This is a classical motivation for the linear regression model.

\section{Normal Regression Model}

The \highlightB{normal regression model} is the linear regression model with an independent normal error
\begin{equation}
    \label{hansen_eq_5_1}
    \begin{aligned}
        & Y = \ora{X}^\prime \ora{\b} + e 
        & e \sim N\of{0, \s^2}.
    \end{aligned}
\end{equation}
As we learned in Section \ref{hansen_sec_5_4} the normal regression model holds when $\bp{Y, \ora{X}}$ are jointly normally distributed. \highlightP{Normal regression, however, does not require joint normality. All that is required is that the conditional distribution of $Y$ given $\ora{X}$ is normal (the marginal distribution of $\ora{X}$ is unrestricted).} In this sense the normal regression model is broader than joint normality. Notice that for notational convenience we have written (\ref{hansen_eq_5_1}) so that $\ora{X}$ contains the intercept.

Normal regression is a parametric model where likelihood methods can be used for estimation, testing, and distribution theory. The \highlightB{likelihood} is the name for the joint probability density of the data, evaluated at the observed sample, and viewed as a function of the parameters. The maximum likelihood estimator is the value which maximizes this likelihood function. Let us now derive the likelihood of the normal regression model.

First, observe that model (\ref{hansen_eq_5_1}) is equivalent to the statement that the conditional density of $Y$ given $\ora{X}$ takes the form
\begin{equation}
    \notag
    f(y \mid \ora{x})=\frac{1}{\left(2 \pi \sigma^2\right)^{1 / 2}} \exp \left(-\frac{1}{2 \sigma^2}\left(y-\ora{x}^{\prime} \ora{\beta}\right)^2\right).
\end{equation}
Under \highlightO{the assumption that the observations are mutually independent} this implies that the conditional density of $\bp{Y_1, \ldots, Y_n}$ given $\bp{\ora{X}_1, \ldots, \ora{X}_n}$ is 
\begin{equation}
	\notag
	\begin{aligned}
		\begin{aligned}
            f\left(y_1, \ldots, y_n \mid \ora{x}_1, \ldots, \ora{x}_n\right) & =\prod_{i=1}^n f\left(y_i \mid \ora{x}_i\right) \\
            & =\prod_{i=1}^n \frac{1}{\left(2 \pi \sigma^2\right)^{1 / 2}} \exp \left(-\frac{1}{2 \sigma^2}\left(y_i-\ora{x}_i^{\prime} \ora{\beta}\right)^2\right) \\
            & =\frac{1}{\left(2 \pi \sigma^2\right)^{n / 2}} \exp \left(-\frac{1}{2 \sigma^2} \sum_{i=1}^n\left(y_i-\ora{x}_i^{\prime} \ora{\beta}\right)^2\right) \\
            & \eqqcolon L_n\left(\ora{\beta}, \sigma^2\right) .
            \end{aligned}
	\end{aligned}
\end{equation}
This is called the \highlightB{likelihood function} when evaluated at the sample data.

For convenience it is typical to work with the natural logarithm 
\begin{equation}
    \label{hansen_eq_5_2}
    \log L_n\left(\ora{\beta}, \sigma^2\right) = -\frac{n}{2}\log\of{2 \pi \s^2} - \frac{1}{2 \s^2} \sum_{i=1}^{n} \bp{Y_i - \ora{X}_i^\prime \ora{\b}}^2 \eqqcolon \ell_n\of{\ora{\b}, \s^2}
\end{equation}
which is called the \highlightB{log-likelihood function}.

The \highlightB{maximum likelihood estimator} (MLE) $\bp{\wh{\oras{\b}}_{\text{mle}}, \wh{\s}^2_{\text{mle}}}$ is the value which maximizes the log-likelihood. We can write the maximization problem as 
\begin{equation}
    \label{hansen_eq_5_3}
    \left(\widehat{\oras{\beta}}_{\text {mle }}, \widehat{\sigma}_{\text {mle }}^2\right)=\underset{\oras{\beta} \in \mathbb{R}^k, \sigma^2>0}{\operatorname{argmax}} \ell_n\left(\ora{\beta}, \sigma^2\right) .
\end{equation}
In most applications of maximum likelihood the MLE must be found by numerical methods. However in the case of the normal regression model we can find an explicit expression for $\wh{\oras{\b}}_{\text{mle}}$ and $\wh{\s}^2_{\text{mle}}$.

\begin{equation}
    \label{hansen_eq_5_4}
    0 = \left. \frac{\partial}{\partial \oras{\beta}} \ell_n\left(\oras{\beta}, \sigma^2\right) \right\vert _{\ora{\beta}=\widehat{\oras{\beta}}_{\text{mle}}, \sigma^2=\widehat{\sigma}_{\text{mle}}^2}=\frac{1}{\widehat{\sigma}_{\text{mle}}^2} \sum_{i=1}^n \ora{X}_i\left(Y_i-\ora{X}_i^{\prime} \widehat{\oras{\beta}}_{\text{mle}}\right)
\end{equation}

\begin{equation}
    \label{hansen_eq_5_5}
    0 = \left. \frac{\partial}{\partial \sigma^2} \ell_n\left(\ora{\beta}, \sigma^2\right) \right\vert _{\ora{\beta}=\widehat{\oras{\beta}}_{\text{mle}}, \sigma^2=\widehat{\sigma}_{\text{mle}}^2}=-\frac{n}{2 \widehat{\sigma}_{\text{mle}}^2}+\frac{1}{2 \widehat{\sigma}_{\text{mle}}^4} \sum_{i=1}^n\left(Y_i-\ora{X}_i^{\prime} \widehat{\oras{\beta}}_{\text{mle}}\right)^2 .
\end{equation}

It follows that the MLE satisfies 
\begin{equation}
    \notag
    \widehat{\oras{\beta}}_{\text{mle}} = \bp{\sum_{i=1}^{n} \ora{X}_i \ora{X}_i^\prime}^{-1} \bp{\sum_{i=1}^{n} \ora{X}_i Y_i} = \wh{\oras{\b}}_{\text{ols}}.
\end{equation}
That is, the MLE for $\ora{\b}$ is algebraically identical to the OLS estimator.

Solving the second FOC (\ref{hansen_eq_5_5}) for $\widehat{\sigma}_{\text{mle}}^2$ we find 
\begin{equation}
    \notag
    \widehat{\sigma}_{\text{mle}}^2 = \frac{1}{n} \sum_{i=1}^{n} \bp{Y_i - \ora{X}_i^\prime \widehat{\oras{\beta}}_{\text{mle}}}^2 = \frac{1}{n} \sum_{i=1}^{n} \bp{Y_i - \ora{X}_i^\prime \widehat{\oras{\beta}}_{\text{ols}}}^2 = \frac{1}{n} \sum_{i=1}^{n} \wh{e}_i^2 = \wh{\s}^2_{\text{ols}}.
\end{equation}
Thus theMLE for $\s^2$ is identical to the OLS/moment estimator from (\ref{hansen_eq_3_26}).

Plugging the estimators into (\ref{hansen_eq_5_2}) we obtain the maximized log-likelihood 
\begin{equation}
    \label{hansen_eq_5_6}
    \ell_n\of{\widehat{\oras{\beta}}_{\text{mle}}, \widehat{\sigma}_{\text{mle}}^2} = -\frac{n}{2} \log\of{2 \pi \widehat{\sigma}_{\text{mle}}^2} - \frac{n}{2}.
\end{equation}
The log-likelihood is typically reported as a measure of fit. 

It may seem surprising that the MLE $\wh{\oras{\b}}_{\text{mle}}$ is numerically equal to the OLS estimator despite emerging from quite different motivations. It is not completely accidental. The least squares estimator minimizes a particular sample loss function -- the sum of squared error criterion -- and \highlightP{most loss functions are equivalent to the likelihood of a specific parametric distribution, in this case the normal regression model}. In this sense it is not surprising that the least squares estimator can be motivated as either the minimizer of a sample loss function or as the maximizer of a likelihood function.


\section{Distribution of OLS Coefficient Vector}

In the normal linear regression model we can derive exact sampling distributions for the OLS/MLE estimator, residuals, and variance estimator. In this section we derive the distribution of the OLS coefficient estimator.

The normality assumption $e \mid \ora{X} \sim N\of{0, \s^2}$ combined with independence of the observations has the multivariate implication 
\begin{equation}
    \notag
    \ora{e} \mid \Xb \sim N\of{\ora{0}, \Ib_n \s^2}.
\end{equation}
That is, the error vector $\ora{e}$ is independent of $\Xb$ and is normally distributed.

Recall that the OLS estimator satisfies 
\begin{equation}
    \notag
    \wh{\oras{\b}} - \ora{\b} = \bp{\Xb^\prime \Xb}^{-1} \Xb^\prime \ora{e}
\end{equation}
which is a linear function of $\ora{e}$. Since linear functions of normals are also normal (Theorem \ref{Hansen_thm_5_2}) this implies that conditional on $\Xb$, 
\begin{equation}
    \notag
    \begin{aligned}
        \wh{\oras{\b}} - \ora{\b} & \mid \boldsymbol{X} \sim\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \mathrm{N}\left(\ora{0}, \boldsymbol{I}_n \sigma^2\right) \\
        & \sim \mathrm{N}\left(\ora{0}, \sigma^2\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right) \\
        & =\mathrm{N}\left(\ora{0}, \sigma^2\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right) .
    \end{aligned}
\end{equation}

\begin{theorem}
    \label{hansen_thm_5_4}
    In the normal regression model, 
    \begin{equation}
        \notag
        \wh{\oras{\b}} \mid \Xb \sim N\of{\ora{\b}, \s^2 \bp{\Xb^\prime \Xb}^{-1}}.
    \end{equation}
\end{theorem}

Any affine function of the OLS estimator is also normally distributed including individual components. Letting $\b_j$ and $\wh{\b}_j$ denote the $j$th element of $\ora{\b}$ and $\wh{\oras{\b}}$, we have 
\begin{equation}
    \label{hansen_eq_5_7}
    \wh{\b}_j \mid \Xb \sim N\of{\b_j, \s^2 \bs{\bp{\Xb^\prime \Xb}^{-1}}_{jj}}.
\end{equation}

Theorem \ref{hansen_thm_5_4} is a statement about the conditional distribution. What about the unconditional distribution? In Theorem \ref{hansen_thm_4_3} we presented Kinal's theorem about the existence of moments for the joint normal regression model. We re-state the result here. 

\begin{theorem}[Kinal (1980)] 
    
    If $(Y, \ora{X})$ are jointly normal, then for any $r$, $\E\bs{\norm{\wh{\oras{b}}}^r} < \infty$ if and only if $r < n - k + 1$.
\end{theorem}

\section{Distribution of OLS Residual Vector}

Consider the OLS residual vector. Recall from (\ref{hansen_eq_3_24}) that $\wh{\oras{e}} = \Mb \ora{e}$ where $\Mb = \Ib_n - \Xb  \bp{\Xb^\prime \Xb}^{-1} \Xb^\prime$. This shows that $\wh{\oras{e}}$ is linear in $\ora{e}$. So conditional on $\Xb$ 
\begin{equation}
    \notag
    \wh{\oras{e}} \mid \Xb  = \Mb \ora{e} \mid \Xb \sim N\of{\ora{0}, \s^2 \Mb \Mb} = N\of{\ora{0}, \s^2 \Mb}
\end{equation}
the final equality since $\Mb$ is idempotent.

Furthermore, it is useful to find the joint distribution of $\wh{\oras{\b}}$ and $\wh{\oras{e}}$. This is easiest done by writing the two as a stacked linear function of the error $\ora{e}$. Indeed, 
\begin{equation}
    \notag
    \left(\begin{array}{c}
        \widehat{\oras{\beta}}-\ora{\beta} \\
        \widehat{\oras{e}}
        \end{array}\right)=\left(\begin{array}{c}
        \left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \ora{e} \\
        \boldsymbol{M} \ora{e}
        \end{array}\right)=\left(\begin{array}{c}
        \left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \\
        \boldsymbol{M}
    \end{array}\right) \ora{e}
\end{equation}
which is a linear function of $\ora{e}$. The vector thus has a joint normal distribution with covariance matrix 
\begin{equation}
    \notag
    \left(\begin{array}{cc}
        \sigma^2\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} & \bds{0} \\
        \bds{0} & \sigma^2 \boldsymbol{M} .
    \end{array}\right)
\end{equation}
The off-diagonal block is zero because $\Xb^\prime \Mb = \bds{0}$. Since this is zero it follows that $\widehat{\oras{\beta}}$ and $\wh{\oras{e}}$ are statistically independent.

\begin{theorem} \label{hansen_thm_5_6}
    In the normal regression model, $\wh{\oras{e}} \mid \Xb \sim N\of{\ora{0}, \s^2 \Mb}$ and is independent of $\wh{\ora{\b}}$.
\end{theorem}

\highlightP{The fact that $\wh{\oras{\b}}$ and $\wh{\oras{e}}$ are independent implies that $\wh{\oras{\b}}$ is independent of any function of the residual vector including individual residuals $\wh{e}_i$ and the variance estimators $s^2$ and $\wh{\s}^2$.}

\section{Distribution of Variance Estimator}

Next, consider the variance estimator $s^2$ defined as 
\begin{equation}
    \notag
    s^2 = \frac{1}{n - k} \sum_{i=1}^{n} \wh{e}_i^2.
\end{equation}
Using (\ref{hansen_eq_3_28}) it satisfies $\bp{n-k} s^2 = \wh{\oras{e}}^\prime \wh{\oras{e}} = \ora{e}^\prime \Mb \ora{e}$.

The spectral decomposition of $\Mb$ is $\Mb = \Hb \bds{\Lambda} \Hb^\prime$ where $\Hb^\prime \Hb = \Ib$ and $\bds{\L}$ is diagonal with the eigenvalues of $\Mb$ on the diagonal. Since $\Mb$ is idempotent with rank $n-k$ it has $n-k$ eigenvalues equalling 1 and $k$ eigenvalues equalling 0, so 
\begin{equation}
    \notag
    \bds{\L} = \left[\begin{array}{cc}
        \boldsymbol{I}_{n-k} & \mathbf{0} \\
        \mathbf{0} & \mathbf{0}_k
        \end{array}\right].
\end{equation}
Let $\ora{u} = \Hb^\prime \ora{e} \sim N\of{\ora{0}, \Ib_n \s^2}$ and partition $\ora{u} = \bp{\ora{u}_1^\prime, \ora{u}_2^\prime}^\prime$. Then 
\begin{equation}
    \notag
    \begin{aligned}
        (n-k) s^2 & =\ora{e}^{\prime} \boldsymbol{M} \ora{e} \\
        & =\ora{e}^{\prime} \boldsymbol{H}\left[\begin{array}{cc}
        \boldsymbol{I}_{n-k} & \mathbf{0} \\
        \mathbf{0} & \mathbf{0}
        \end{array}\right] \boldsymbol{H}^{\prime} \ora{e} \\
        & =\ora{u}^{\prime}\left[\begin{array}{cc}
        \boldsymbol{I}_{n-k} & \mathbf{0} \\
        \mathbf{0} & \mathbf{0}
        \end{array}\right] \ora{u} \\
        & =\ora{u}_1^{\prime} \ora{u}_1 \\
        & \sim \sigma^2 \chi_{n-k}^2 .
    \end{aligned}
\end{equation}
We see that in the normal regression model the exact distribution of $s^2$ is scaled chi-squared.

Since $\wh{\oras{e}}$ is independent of $\wh{\oras{\b}}$ it follows that $s^2$ is independent of $\wh{\ora{\b}}$ as well. 
\begin{theorem}
    \label{hansen_thm_5_7}
    In the normal regression model, 
    \begin{equation}
        \notag
        \frac{\bp{n-k} s^2}{\s^2} \sim \chi_{n-k}^2 ,
    \end{equation}
    and is independent of $\wh{\oras{\b}}$.
\end{theorem}

\section{$t$-Statistic}

An alternative way of writing (\ref{hansen_eq_5_7}) is 
\begin{equation}
    \notag
    \frac{\wh{\b}_j - \b_j}{\sqrt{ \s^2 \bs{\bp{\Xb^\prime \Xb}^{-1}}_{jj}}} \sim N\of{0,1}.
\end{equation}
This is sometimes called a \highlightB{standardized} statistic as the distribution is the standard normal.

Now take the standardized statistic and replace the unknown variance $\s^2$ with its estimator $s^2$. We call this a \highlightB{$t$-ratio} or \highlightB{$t$-statistic}
\begin{equation}
    \notag
    T = \frac{\wh{\b}_j - \b_j}{\sqrt{ s^2 \bs{\bp{\Xb^\prime \Xb}^{-1}}_{jj}}} = \frac{\wh{\b}_j - \b_j}{s\of{\wh{\b}_j}}, 
\end{equation}
where $s\of{\wh{\b}_j}$ is the classical (homoskedastic) standard error for $\wh{\b}_j$ from (4.37). We will sometimes write the $t$-statistic as $T\of{\b_j}$ to explicitly indicate its dependence on the parameter value $\b_j$, and sometimes will simplify notation and write the $t$-statistic as $T$ when the dependence is clear from the context.

With algebraic re-scaling we can write the t-statistic as the ratio of the standardized statistic and the square root of the scaled variance estimator. Since the distributions of these two components are normal and chi-square, respectively, and independent, we deduce that the $t$-statistic has the distribution
\begin{equation}
    \notag
    \begin{aligned}
        T & =\frac{\widehat{\beta}_j-\beta_j}{\sqrt{\sigma^2\left[\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right]_{j j}}} / \sqrt{\frac{(n-k) s^2}{\sigma^2} /(n-k)} \\
        & \sim \frac{\mathrm{N}(0,1)}{\sqrt{\chi_{n-k}^2 /(n-k)}} \\
        & \sim t_{n-k},
    \end{aligned}
\end{equation}
a student $t$ distribution with $n - k$ degrees of freedom.

\highlightR{This derivation shows that the $t$-ratio has a sampling distribution which depends only on the quantity $n - k$. The distribution does not depend on any other features of the data.} In this context, we say that the distribution of the $t$-ratio is \highlightB{pivotal}, meaning that it does not depend on unknowns.

\highlightP{The trick behind this result is scaling the centered coefficient by its standard error, and recognizing that each depends on the unknown $\s$ only through scale.} Thus the ratio of the two does not depend on $\s$. This trick (scaling to eliminate dependence on unknowns) is known as studentization.

\begin{theorem}
    \label{hansen_thm_5_8}
    In the normal regression model, $ T \sim t_{n-k}$.
\end{theorem}

\highlightP{An important caveat about Theorem \ref{hansen_thm_5_8} is that it only applies to the $t$-statistic constructed with the homoskedastic (old-fashioned) standard error. It does not apply to a $t$-statistic constructed with any of the robust standard errors.} In fact, the robust $t$-statistics can have finite sample distributions which deviate considerably from $t_{n-k}$ even when the regression errors are independent $N\of{0, \s^2}$. \highlightR{Thus the distributional result in Theorem \ref{hansen_thm_5_8} and the use of the $t$ distribution in finite samples is only exact when applied to classical $t$-statistics under the normality assumption.}

\section{Confidence Intervals for Regression Coefficients} \label{hansen_sec_5_10}

The OLS estimator $\wh{\oras{\b}}$ is a \highlightB{point estimator} for a coefficient $\ora{\b}$. A broader concept is a \highlightB{set} or \highlightB{interval estimator} which takes the form $\wh{C} = \bs{\wh{L}, \wh{U}}$. The goal of an interval estimator $\wh{C}$ is to contain the true value, e.g., $\ora{\b} \in \wh{{C}}$ with high probability.

The interval estimator $\wh{C}$ is a function of the data and hence is random.

An interval estimator $\wh{C}$ is called a $1 - \a$ \highlightB{confidence interval} when $\P\bs{\ora{\b} \in \wh{{C}}} = 1 - \a$ for a selected value of $\a$. The value $1 - \a$ is called the \highlightB{coverage probability}. 

The probability calculation $\P\bs{\ora{\b} \in \wh{C}}$ is easily mis-interpreted as treating $\ora{\b}$ as random and $\wh{C}$ as fixed. (The probability that $\ora{\b}$ is in $\wh{C}$.) This is not the appropriate interpretation. \highlightP{Instead, the correct interpretation is that the probability $\P\bs{\ora{\b} \in \wh{C}}$ treats the point $\ora{\b}$ as fixed and the set $\wh{C}$ as random. It is the probability that the random set $\wh{C}$ covers (or contains) the fixed true coefficient $\ora{\b}$.}

There is not a unique method to construct confidence intervals. For example, one simple (yet silly) interval is 
\begin{equation}
    \notag 
    \wh{C} = \begin{cases}
        \R & \text{with probability } 1-\a \\
        \bc{\wh{\oras{\b}}} & \text{with probability } \a .
    \end{cases}
\end{equation}
If $\wh{\oras{\b}}$ has a continuous distribution, then by construction $\P\bs{\ora{\b} \in \wh{C}} = 1-\a$, so this confidence interval has perfect coverage. However, $\wh{C}$ is uninformative and is therefore not useful.

A good choice for a confidence interval for the regression coefficient $\ora{\b}$ is obtained by adding and subtracting from the estimator $\wh{\oras{\b}}$ a fixed multiple of its standard error:
\begin{equation}
    \label{hansen_eq_5_8}
    \wh{{C}} = \bs{\wh{\oras{\b}} - c \times s\of{\wh{\oras{\b}}}, \wh{\oras{\b}} + c \times s\of{\wh{\oras{\b}}}}
\end{equation}
where $c > 0$ is a pre-specified constant. This confidence interval is symmetric about the point estimator $\wh{\oras{\b}}$ and its length is proportional to the standard error $s\of{\wh{\oras{\b}}}$.

Equivalently, $\wh{{C}}$ is the set of parameter values for $\ora{\b}$ such that the $t$-statistic $T\of{\b}$ is smaller (in absolute value) than $c$, that is 
\begin{equation}
    \notag
    \wh{{C}} = \bc{\ora{\b}: \abs{T\of{\ora{\b}}} \leq c} = \bc{\ora{\b}: -c \leq \frac{\wh{\oras{\b}} - \ora{\b}}{s\of{\wh{\oras{\b}}}} \leq c}.
\end{equation}

The coverage probability of this confidence interval is 
\begin{equation}
    \label{hansen_eq_5_9}
    \begin{aligned}
        \mathbb{P}[\ora{\b} \in \widehat{{C}}] & =\mathbb{P}\bs{\abs{T(\ora{\b})} \leq c} \\
        & =\mathbb{P}\bs{-c \leq T(\ora{\b}) \leq c} .
        \end{aligned}
\end{equation}
Since the $t$-statistic $T\of{\ora{\b}}$ has the $t_{n-k}$ distribution (\ref{hansen_eq_5_9}) equals $F\of{c} - F\of{-c}$, where $F\of{u}$ is the student $t$ distribution function with $n-k$ degrees of freedom. Since $F\of{-c} - 1 - F\of{c}$ we can write (\ref{hansen_eq_5_9}) as 
\begin{equation}
    \notag
    \P\bs{\ora{\b} \in \wh{{C}}} = 2 F\of{c} - 1.
\end{equation}
This is the coverage probability of the interval $\wh{{C}}$, and only depends on the constant $c$.

As we mentioned before, a confidence interval has the coverage probability $1-\a$. This requires selecting the constant $c$ so that $F\of{c} = 1 - \a/2$. This holds if $c$ equals the $1-\a/2$ quantile of the $t_{n-k}$ distribution. As there is no closed form expression for these quantiles we compute their values numerically. \highlightP{By default, Stata reports 95\% confidence intervals $\wh{C}$ for each estimated regression coefficient using the same formula.}

\begin{theorem}
    \label{hansen_thm_5_9}
    In the normal regression model, the confidence interval constructed in (\ref{hansen_eq_5_8}), i.e.,
    $$
    \wh{{C}} = \bs{\wh{\oras{\b}} - c \times s\of{\wh{\oras{\b}}}, \wh{\oras{\b}} + c \times s\of{\wh{\oras{\b}}}}
    $$
    with $c = F^{-1}\of{1 - \a/2}$ has coverage probability $\P\bs{\ora{\b} \in \wh{{C}}} = 1 - \a$.
\end{theorem}

When the degree of freedom is large the distinction between the student $t$ and the normal distribution is negligible. In particular, for $n-k \geq 61$, we have $c \leq 2.00$ for a 95\% interval. Using this value we obtain the most commonly used confidence interval in applied econometric practice:
\begin{equation}
    \label{hansen_eq_5_10}
    \wh{{C}} = \bs{\wh{\oras{\b}} - 2 s\of{\wh{\oras{\b}}}, \wh{\oras{\b}} + 2 s\of{\wh{\oras{\b}}}}
\end{equation}
This is a useful rule-of-thumb.

\begin{theorem}
    \label{hansen_thm_5_10}
    In the normal regression model, if $n-k \geq 61$, then (\ref{hansen_eq_5_10}) has coverage probability $\P\bs{\ora{\b} \in \wh{{C}}} \geq 0.95$.
\end{theorem}


\section{Confidence Intervals for Error Variance}

We can also construct a confidence interval for the regression error variance $\s^2$ using the sampling distribution of $s^2$ from Theorem \ref{hansen_thm_5_7}. This states that in the normal regression model
\begin{equation}
    \label{hansen_eq_5_11}
    \frac{\bp{n-k}s^2}{\s^2} \sim \chi_{n-k}^2.
\end{equation}
Let $F\of{u}$ denote the $\chi_{n-k}^2$ distribution function and for some $\a$ set $c_1 = F^{-1}\of{\a/2}$ and $c_2 = F^{-1}\of{1 - \a/2}$. Equation (\ref{hansen_thm_5_11}) implies that
\begin{equation}
    \notag
    \P\bs{c_1 \leq \frac{\bp{n-k}s^2}{\s^2} \leq c_2} = 1 - \a.
\end{equation}
Rewriting the inequalities we find 
\begin{equation}
    \notag
    \P\bs{\frac{\bp{n-k}s^2}{c_1} \leq {\s^2} \leq \frac{\bp{n-k}s^2}{c_2}} = 1 - \a.
\end{equation}
This shows that an exact $1 - \a$ confidence interval for $\s^2$ is 
\begin{equation}
    \label{hansen_eq_5_12}
    \wh{C} = \bs{\frac{\bp{n-k}s^2}{c_1}, \frac{\bp{n-k}s^2}{c_2}}.
\end{equation}

\begin{theorem}
    \label{hansen_thm_5_11}
    In the normal regression model, (\ref{hansen_eq_5_12}) has coverage probability $\P\bs{\s^2 \in \wh{{C}}} = 1 - \a$.
\end{theorem}

\section{$t$ Test}

A typical goal in an econometric exercise is to assess whether or not a coefficient $\ora{\b}$ equals a specific value $\ora{\b}_0$. Often the specific value to be tested is $\ora{\b}_0 = \ora{0}$, but this is not essential. This is called \highlightB{hypothesis testing}. In this section and the following we give a short introduction specific to the normal regression model.

For simplicity write the coefficient to be tested as $\b$. The null hypothesis is 
\begin{equation}
    \label{hansen_eq_5_13}
    \mathbb{H}_0: \b = \b_0.
\end{equation}
This states that the hypothesis is that the true value of $\b$ equals to the hypothesized value $\b_0$.

The alternative hypothesis is the complement of $\mathbb{H}_0$, and is written as
$$\mathbb{H}_1: \b \neq \b_0.$$
This states that the true value of $\b_0$ does not equal to the hypothesized value.

The standard statistic to test $\mathbb{H}_0$ against $\mathbb{H}_1$ is the absolute value of the $t$-statistic
\begin{equation}
    \label{hansen_eq_5_14}
    \abs{T} = \abs{\frac{\wh{{\b}} - {\b}}{s\of{\wh{{\b}}}}}.
\end{equation}
If $\mathbb{H}_0$ is true then we expect $\abs{T}$ to be small, but if $\mathbb{H}_1$ is true then we expect $\abs{T}$ to be large. Hence the standard rule is to reject $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ for large values of the $t$-statistic $\abs{T}$ and otherwise fail to reject $\mathbb{H}_0$. Thus, the hypothesis test takes the form
\begin{equation}
    \notag
    \text{Reject  } \mathbb{H}_0 \text{  if  } \abs{T} > c.
\end{equation}

The constant $c$ which appears in the statement of the test is called the \highlightB{critical value}. Its value is selected to control the probability of false rejections. When the null hypothesis is true $T$ has an exact $t_{n-k}$ distribution in the normal regression model. Thus for a given value of $c$ the probability of false rejection is
\begin{equation}
    \notag
    \begin{aligned}
        \mathbb{P}\left[\text { Reject } \mathbb{H}_0 \mid \mathbb{H}_0\right] & =\mathbb{P}\left[|T|>c \mid \mathbb{H}_0\right] \\
        & =\mathbb{P}\left[T>c \mid \mathbb{H}_0\right]+\mathbb{P}\left[T<-c \mid \mathbb{H}_0\right] \\
        & =1-F(c)+F(-c) \\
        & =2(1-F(c)),
    \end{aligned}
\end{equation}
where $F\of{u}$ is the $t_{n-k}$ distribution function. \highlightP{This is the probability of false rejection and is decreasing in the critical value $c$.} We select the value $c$ so that this probability equals a pre-selected value called the \highlightB{significance level} which is typically written as $\a$. It is conventional to set $\a = 0.05$, though this is not a hard rule. We then select $c$ so that $F\of{c} = 1 - \a/2$, which means that $c$ is the $1-  \a/2$ quantile of the $t_{n-k}$ distribution, the same as used for confidence intervals. With this choice the decision rule ``Reject $\mathbb{H}_0$ if $\abs{T} > c$'' has a significance level (false rejection probability) of $\a$.

\begin{theorem}
    \label{hansen_thm_5_12}
    In the normal regression model if the null hypothesis (\ref{hansen_eq_5_13}) is true, then for $\abs{T}$ defined in (\ref{hansen_eq_5_14}) $T \sim t_{n-k}$. If $c$ is set so that $\P\bs{\abs{t_{n-k}} \geq c} = \a$ then the test ``Reject $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ if $\abs{T} > c$'' has significance level $\a$.
\end{theorem}

To report the result of a hypothesis test we need to pre-determine the significance level $\a$ in order to calculate the critical value c. This can be inconvenient and arbitrary. In general, when a test takes the form ``Reject $\mathbb{H}_0$ if $S > c$'' and $S$ has null distribution $G\of{u}$ then the $p$-value of the test is $p = 1 - G\of{c}$. A simplification is to report what is known as the \highlightB{$p$-value} of the test. It is sufficient to report the $p$-value $p$ and \highlightP{we can interpret the value of $p$ as indexing the test's strength of rejection of the null hypothesis}. Thus a $p$-value of $0.07$ might be
interpreted as ``nearly significant'', $0.05$ as ``borderline significant'', and $0.001$ as ``highly significant''. In the context of the normal regression model the $p$-value of a $t$-statistic $\abs{T}$ is $p = 2\bp{1- F_{n-k}\of{\abs{T}}}$ where $F_{n-k}$ is the $t_{n-k}$ CDF.

\highlightP{A $p$-value reports the strength of evidence against $\mathbb{H}_0$ but is not itself a probability.} A common mistake understanding is that the $p$-value is the ``probability that the null hypothesis is true.'' This is incorrect interpretation. It is a static, is random, and is a measure of the evidence against $\mathbb{H}_0$. Nothing more.

\section{Likelihood Ratio Test}

In the previous section we described the $t$-test as the standard method to test a hypothesis on a single coefficient in a regression. In many contexts, however, we want to simultaneously assess a set of coefficients. In the normal regression model, this can be done by an $F$ test which can be derived from the likelihood ratio test.

Partition the regressors as $\ora{X} = \bp{\ora{X}_1^{\prime}, \ora{X}_2^{\prime}}^{\prime}$ and similarly partition the coefficient vector as $\ora{\b} = \bp{\ora{\b}_1^{\prime}, \ora{\b}_2^{\prime}}^{\prime}$. The regression model can be written as 
\begin{equation}
    \label{hansen_eq_5_15}
    Y = \ora{X}_1 \ora{\b}_1 + \ora{X}_2 \ora{\b}_2 + e.
\end{equation}
Let $k = \dim\of{\ora{X}}, k_1 = \dim\of{\ora{X}_1}, q = \dim\of{\ora{X}_2}$, so that $k = k_1 + q$. Partition the variables so that the hypothesis is that the second set of coefficients are zero, or 
\begin{equation}
    \label{hansen_eq_5_16}
    \mathbb{H}_0: \ora{\b}_2 = \ora{0}.
\end{equation}
If $\mathbb{H}_0$ is true then the regressors $\ora{X}_2$ can be omitted from the regression. In this case we can write (\ref{hansen_eq_5_15}) as 
\begin{equation}
    \label{hansen_eq_5_17}
    Y = \ora{X}_1 \ora{\b}_1 + e.
\end{equation}
We call (\ref{hansen_eq_5_17}) the \highlightB{null model}. The alternative hypothesis is that at least one element of $\ora{\b}_2$ is non-zero and is written as $\mathbb{H}_1: \ora{\b}_2 \neq \ora{0}$.

When models are estimated by maximum likelihood a well-accepted testing procedure is to reject $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ for large values of the \highlightP{Likelihood Ratio} -- the ratio of the maximized likelihood function under $\mathbb{H}_1$ and $\mathbb{H}_0$, respectively. We now construct this statistic in the normal regression model. Recall from (\ref{hansen_eq_5_6}) that the maximized log-likelihood equals
\begin{equation}
    \notag
    \ell_n\of{\widehat{\oras{\beta}}_{\text{mle}}, \widehat{\sigma}_{\text{mle}}^2} = -\frac{n}{2} \log\of{2 \pi \widehat{\sigma}_{\text{mle}}^2} - \frac{n}{2}.
\end{equation}
We similarly calculate the maximized log-likelihood for the constrained model (\ref{hansen_eq_5_17}). By the same steps for derivation of the unconstrained MLE we find that the MLE for (\ref{hansen_eq_5_17}) is OLS of $Y$ on $\ora{X}_1$. We can write the estimator as 
\begin{equation}
    \notag 
    \wt{\oras{\b}}_1 = \bp{\ora{X}_1^\prime \ora{X}_1}^{-1} \ora{X}_1^\prime Y ,
\end{equation}
with residual $\wt{e}_i = Y_i - \ora{X}_1^{\prime} \wt{\oras{\b}}_1$ and the error variance estimator $\wt{\s}^2 = \frac{1}{n} \sum_{i=1}^{n} \wt{e}_i^2$. You can calculate similar to (\ref{hansen_eq_5_6}) that the maximized constrained log-likelihood is 
\begin{equation}
    \notag 
    \ell_n\of{\wt{\oras{\b}}_1, \wt{\s}^2} = -\frac{n}{2} \log\of{2 \pi \wt{\s}^2} - \frac{n}{2}.
\end{equation}

A classic testing procedure is to reject $\mathbb{H}_0$ for large values of the ratio of the maximized likelihoods. Equivalently the test rejects $\mathbb{H}_0$ for large values of the twice the difference in the log-likelihood functions. (Multiplying the likelihood difference by two turns out to be a useful scaling.) This equals
\begin{equation}
    \label{hansen_eq_5_18}
    \begin{aligned}
        LR & = 2 \bp{\ell_n\of{\wh{\oras{\b}}_1, \wh{\s}^2} - \ell_n\of{\wt{\oras{\b}}, \wt{\s}^2}} \\
        & = n \log \of{\frac{\wt{\s}^2}{\wh{\s}^2}}.
    \end{aligned}
\end{equation}
The likelihood ratio test rejects $\mathbb{H}_0$ for large values of $LR$, or equivalently for large values of\footnote{There is a one-to-one mapping between $LR$ and $F$:
\begin{equation}
    \notag 
    F = \bs{\exp\of{\frac{LR}{n}} - 1} \times \frac{n-k}{q}.
\end{equation}
}
\begin{equation}
    \label{hansen_eq_5_19}
    F = \frac{\bp{\wt{\s}^2 - \wh{\s}^2} / q}{\wh{\s}^2/\bp{n-k}}.
\end{equation}
This is known as the $F$ statistic for the test of hypothesis $\mathbb{H}_0$ against $\mathbb{H}_1$.

To develop an appropriate critical value we need the null distribution of $F$. Recall from (\ref{hansen_eq_3_28}) that $n \wh{\s}^2 = \ora{e}^{\prime} \Mb \ora{e}$ where $\Mb = \Ib_n - \Pb$ with $\Pb = \Xb \bp{\Xb^{\prime} \Xb}^{-1}\Xb^{\prime}$. Similarly, under $\mathbb{H}_0$, $n \wt{\s}^2 = \ora{e}^{\prime} \Mb_1 \ora{e}$ where $\Mb_1 = \Ib_n - \Pb_1$ with $\Pb_1 = \Xb_1 \bp{\Xb_1^{\prime} \Xb_1}^{-1}\Xb_1^{\prime}$. You can calculate that $\Mb_1 - \Mb = \Pb - \Pb_1$ is idempotent with rank $q$. Furthermore, $\bp{\Mb_1 - \Mb} \Mb = \bds{0}$. It follows that 
$$\ora{e}^{\prime} \bp{\Mb_1 - \Mb} \ora{e} \sim \chi_q^2$$
and is independent of $\ora{e}^{\prime}\Mb\ora{e}$. Hence 
\begin{equation}
    \notag
    F=\frac{\ora{e}^{\prime}\left(\boldsymbol{M}_1-\boldsymbol{M}\right) \ora{e} / q}{\ora{e}^{\prime} \boldsymbol{M} \ora{e} /(n-k)} \sim \frac{\chi_q^2 / q}{\chi_{n-k}^2 /(n-k)} \sim F_{q, n-k},
\end{equation}
an exact $F$ distribution with degrees of freedom $q$ and $n-k$, respectively. Thus under $\mathbb{H}_0$, the $F$ statistic has an exact $F_{q, n-k}$ distribution.

The critical values are selected from the upper tail of the $F$ distribution. For a given significance level
$\a$ (typically $\a = 0.05$) we select the critical value $c$ so that
$$
\P\bs{F_{q, n-k} \geq c} = \a.
$$
The test rejects $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ if $F > c$ and does not reject $\mathbb{H}_0$ otherwise. The p-value of the test is $p = 1 - G_{q, n-k}\of{F}$) where $G_{q, n-k}\of{\cdot}$ is the $F_{q, n-k}$ distribution function.

\begin{theorem}
    \label{hansen_thm_5_13}
    In the normal regression model if the null hypothesis (\ref{hansen_eq_5_16}) is true then for $F$ defined in (\ref{hansen_eq_5_19}), i.e.,
    \begin{equation}
        \notag 
        F = \frac{\bp{\wt{\s}^2 - \wh{\s}^2} / q}{\wh{\s}^2/\bp{n-k}}.
    \end{equation}
    $F \sim F_{q, n-k}$. If $c$ is set so that $\P\bs{F_{q, n-k} \geq c} = \a$ then the test ``Reject $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ if $F > c$'' has significance level $\a$.
\end{theorem}

\highlightPP{The derivation of the $F$ distribution?}

\section{Information Bound for Normal Regression}

The likelihood scores for the normal regression model are 
\begin{equation}
    \notag 
    \frac{\partial}{\partial \oras{\b}} \ell_n\of{\ora{\b}, \s^2} = \frac{1}{\s^2} \sum_{i=1}^{n} \ora{X}_i \bp{Y_i - \ora{X}_i^{\prime} \ora{\b}} = \frac{1}{\s^2} \sum_{i=1}^{n} \ora{X}_i \ora{e}_i 
\end{equation}
and 
\begin{equation}
    \notag 
    \frac{\partial}{\partial \sigma^2} \ell_n\left(\oras{\b}, \sigma^2\right)=-\frac{n}{2 \sigma^2}+\frac{1}{2 \sigma^4} \sum_{i=1}^n\left(Y_i-\ora{X}_i^{\prime} \ora{\beta}\right)^2=\frac{1}{2 \sigma^4} \sum_{i=1}^n\left(e_i^2-\sigma^2\right)
\end{equation}
It follows that the information matrix is 
\begin{equation}
    \notag 
    \mathscr{I}=\operatorname{var}\left[\begin{array}{c|c}
        \frac{\partial}{\partial \oras{\beta}} \ell\left(\oras{\beta}, \sigma^2\right) & \boldsymbol{X} \\
        \frac{\partial}{\partial \sigma^2} \ell\left(\oras{\beta}, \sigma^2\right) & \boldsymbol{X}
        \end{array}\right]=\left(\begin{array}{cc}
        \frac{1}{\sigma^2} \boldsymbol{X}^{\prime} \boldsymbol{X} & 0 \\
        0 & \frac{n}{2 \sigma^4}
        \end{array}\right)
\end{equation}
The Cramér-Rao Lower Bound is 
\begin{equation}
    \notag 
    \mathscr{I}^{-1} = \left(\begin{array}{cc}
        \sigma^2 \bp{\Xb^{\prime} \Xb}^{-1} & 0 \\
        0 & \frac{n}{2 \s^4}
        \end{array}\right)
\end{equation}
This shows that the lower bound for estimation of $\ora{\b}$ is $\s_2 \bp{\Xb^{\prime} \Xb}^{-1}$ and the lower bound for $\s_2$ is $2\s^4 / n$.

Since in the homoskedastic linear regression model the OLS estimator is unbiased and has variance $\s^2 \bp{\Xb^{\prime} \Xb}^{-1}$, \highlightP{it follows that the OLS coefficient estimator $\wh{\oras{\b}}$ is Cramér-Rao efficient in the normal regression model}. \highlightB{Cramér-Rao efficiency means that no unbiased estimator has a lower covariance matrix.} This expands on the Gauss-Markov theorem which stated that no linear unbiased estimator has a lower variance matrix in the homoskedastic regression model. Notice that that the results are complementary. \highlightP{Gauss-Markov efficiency concerns a more narrow class of estimators (linear) but allows a broader model class (linear homoskedastic rather than normal regression). The Cramér-Rao efficiency result is more powerful in that it does not restrict the class of estimators (beyond unbiasedness) but is more restrictive in the class of models allowed (normal regression).}

The unbiased variance estimator $s^2$ of $\s^2$ has variance $2 \s^4 / \bp{n-k}$, which is larger than the Cramér-Rao lower bound $2 \s^4 / n$. Thus in contrast to the coefficient estimator, the variance estimator is not Cramér-Rao efficient. 